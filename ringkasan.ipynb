{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengumpulan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pembersihan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, to_date, concat_ws\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "# Membuat SparkSession\n",
    "spark = SparkSession.builder.appName(\"PreprocessingNewsData\").getOrCreate()\n",
    "\n",
    "# Membaca data dari file CSV\n",
    "df = spark.read.csv(\"hdfs://hadoop-namenode:8020/user/news_data/articles_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Membersihkan teks dengan menghapus karakter khusus dari kolom title, reporter, editor, dan content\n",
    "df_clean = df.withColumn(\"title\", regexp_replace(col(\"title\"), \"[^a-zA-Z0-9\\\\s]\", \"\")) \\\n",
    "             .withColumn(\"reporter\", regexp_replace(col(\"reporter\"), \"[^a-zA-Z0-9\\\\s]\", \"\")) \\\n",
    "             .withColumn(\"editor\", regexp_replace(col(\"editor\"), \"[^a-zA-Z0-9\\\\s]\", \"\")) \\\n",
    "             .withColumn(\"content\", regexp_replace(col(\"content\"), \"[^a-zA-Z0-9\\\\s]\", \"\"))\n",
    "\n",
    "# Konversi kolom date_time ke format tanggal\n",
    "df_clean = df_clean.withColumn(\"date_time\", to_date(col(\"date_time\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Tokenisasi teks pada kolom title dan content\n",
    "tokenizer_title = Tokenizer(inputCol=\"title\", outputCol=\"title_words\")\n",
    "tokenizer_content = Tokenizer(inputCol=\"content\", outputCol=\"content_words\")\n",
    "\n",
    "df_tokenized = tokenizer_title.transform(df_clean)\n",
    "df_tokenized = tokenizer_content.transform(df_tokenized)\n",
    "\n",
    "# Menghapus stopwords dari kolom title dan content\n",
    "remover_title = StopWordsRemover(inputCol=\"title_words\", outputCol=\"title_filtered\")\n",
    "remover_content = StopWordsRemover(inputCol=\"content_words\", outputCol=\"content_filtered\")\n",
    "\n",
    "df_preprocessed = remover_title.transform(df_tokenized)\n",
    "df_preprocessed = remover_content.transform(df_preprocessed)\n",
    "\n",
    "# Mengonversi kolom array menjadi string menggunakan concat_ws\n",
    "df_result = df_preprocessed.withColumn(\"title_filtered\", concat_ws(\" \", col(\"title_filtered\"))) \\\n",
    "                           .withColumn(\"content_filtered\", concat_ws(\" \", col(\"content_filtered\")))\n",
    "\n",
    "# Menampilkan hasil akhir preprocessing\n",
    "df_result.select(\"title_filtered\", \"reporter\", \"editor\", \"date_time\", \"content_filtered\").show(5, truncate=False)\n",
    "\n",
    "# Menyimpan hasil preprocessing ke HDFS sebagai CSV\n",
    "output_path = \"hdfs://hadoop-namenode:8020/user/news_data/hasilpreprocessing.csv\"\n",
    "\n",
    "df_result.select(\"title_filtered\", \"reporter\", \"editor\", \"date_time\", \"content_filtered\") \\\n",
    "         .write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "#df = spark.read.csv(\"hdfs://hadoop-namenode:8020/user/news_data/articles_data.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pemrosesan, Modeling, dan Visualisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import col, concat_ws, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Membuat SparkSession\n",
    "spark = SparkSession.builder.appName(\"TFIDFClusteringWithSentiment\").getOrCreate()\n",
    "\n",
    "# --- 1. Membaca Data ---\n",
    "input_path = \"hdfs://hadoop-namenode:8020/user/news_data/hasilpreprocessing.csv\"\n",
    "df_result = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "\n",
    "# Menggabungkan kata-kata yang telah difilter dari kolom title dan content\n",
    "df_filtered = df_result.withColumn(\"filtered_words\", concat_ws(\" \", col(\"title_filtered\"), col(\"content_filtered\")))\n",
    "\n",
    "# Menampilkan 5 baris teratas setelah menggabungkan kata-kata yang difilter\n",
    "df_filtered.select(\"filtered_words\").show(5, truncate=False)\n",
    "\n",
    "# --- 2. Tokenisasi dan TF-IDF ---\n",
    "# Tokenisasi\n",
    "tokenizer_filtered = Tokenizer(inputCol=\"filtered_words\", outputCol=\"words\")\n",
    "df_tokenized_filtered = tokenizer_filtered.transform(df_filtered)\n",
    "\n",
    "# Menampilkan 5 baris teratas setelah tokenisasi\n",
    "df_tokenized_filtered.select(\"filtered_words\", \"words\").show(5, truncate=False)\n",
    "\n",
    "# HashingTF\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "featurizedData = hashingTF.transform(df_tokenized_filtered)\n",
    "\n",
    "# Menampilkan 5 baris teratas setelah HashingTF\n",
    "featurizedData.select(\"words\", \"rawFeatures\").show(5, truncate=False)\n",
    "\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "# Menampilkan 5 baris teratas setelah IDF\n",
    "rescaledData.select(\"rawFeatures\", \"features\").show(5, truncate=False)\n",
    "\n",
    "# Konversi kolom features menjadi array float\n",
    "def vector_to_array(v):\n",
    "    return v.toArray().tolist()\n",
    "\n",
    "vector_to_array_udf = udf(vector_to_array, ArrayType(FloatType()))\n",
    "rescaledData = rescaledData.withColumn(\"features_array\", vector_to_array_udf(col(\"features\")))\n",
    "\n",
    "# --- 3. Menentukan Sentimen ---\n",
    "positive_words = ['baik', 'bagus', 'positif', 'hebat', 'senang', 'puas', 'luar biasa']\n",
    "negative_words = ['buruk', 'negatif', 'jelek', 'sedih', 'kecewa', 'mengecewakan', 'payah']\n",
    "\n",
    "def determine_sentiment(text):\n",
    "    text_lower = text.lower()\n",
    "    if any(word in text_lower for word in positive_words):\n",
    "        return 'positive'\n",
    "    elif any(word in text_lower for word in negative_words):\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "determine_sentiment_udf = udf(determine_sentiment, StringType())\n",
    "df_with_sentiment = rescaledData.withColumn(\"sentiment\", determine_sentiment_udf(col(\"filtered_words\")))\n",
    "\n",
    "# --- 4. K-Means Clustering ---\n",
    "kmeans = KMeans(k=3, seed=1, featuresCol=\"features\", predictionCol=\"cluster\")\n",
    "model = kmeans.fit(df_with_sentiment)\n",
    "df_with_cluster = model.transform(df_with_sentiment)\n",
    "\n",
    "# Evaluasi clustering\n",
    "evaluator = ClusteringEvaluator(predictionCol=\"cluster\", featuresCol=\"features\")\n",
    "silhouette = evaluator.evaluate(df_with_cluster)\n",
    "print(f\"Silhouette Score: {silhouette}\")\n",
    "\n",
    "# --- 5. PCA untuk Visualisasi ---\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "df_pca = pca.fit(df_with_cluster).transform(df_with_cluster)\n",
    "\n",
    "# Ekstraksi nilai PCA\n",
    "def extract_pca_values(vector):\n",
    "    return vector.toArray().tolist()\n",
    "\n",
    "extract_pca_values_udf = udf(extract_pca_values, ArrayType(FloatType()))\n",
    "df_pca = df_pca.withColumn(\"pca_values\", extract_pca_values_udf(col(\"pca_features\")))\n",
    "\n",
    "# Ambil komponen PCA pertama dan kedua\n",
    "df_pca = df_pca.withColumn(\"x\", col(\"pca_values\")[0])\n",
    "df_pca = df_pca.withColumn(\"y\", col(\"pca_values\")[1])\n",
    "\n",
    "# Konversi ke Pandas untuk visualisasi\n",
    "df_pca_pd = df_pca.select(\"x\", \"y\", \"cluster\", \"sentiment\").toPandas()\n",
    "\n",
    "# --- 6. Visualisasi Hasil Clustering dengan Sentimen ---\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Scatter plot dengan warna berdasarkan cluster\n",
    "scatter = plt.scatter(df_pca_pd['x'], df_pca_pd['y'], c=df_pca_pd['cluster'], cmap='viridis', marker='o')\n",
    "\n",
    "# Tambahkan judul dan label sumbu\n",
    "plt.title(\"Visualisasi Hasil Clustering dengan K-Means dan Sentimen (PCA 2D)\")\n",
    "plt.xlabel(\"PCA Komponen 1\")\n",
    "plt.ylabel(\"PCA Komponen 2\")\n",
    "\n",
    "# Tambahkan colorbar untuk cluster\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "\n",
    "# Simpan plot sebagai file PNG\n",
    "plt.savefig(\"clustering_sentiment_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "# --- 7. Visualisasi Sentimen dalam Bentuk Pie Chart ---\n",
    "\n",
    "# Hitung jumlah sentimen\n",
    "sentiment_counts = df_pca_pd['sentiment'].value_counts()\n",
    "\n",
    "# Membuat pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140, colors=['#66b3ff', '#ff9999', '#99ff99'])\n",
    "plt.title(\"Distribusi Sentimen dalam Clustering\")\n",
    "\n",
    "# Simpan pie chart sebagai file PNG\n",
    "plt.savefig(\"sentiment_distribution_pie.png\")\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
