{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengumpulan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0',\n",
    "]\n",
    "\n",
    "def get_random_user_agent():\n",
    "    return random.choice(USER_AGENTS)\n",
    "\n",
    "def request_with_retries(url, retries=3, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            headers = {'User-Agent': get_random_user_agent()}\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()  # Untuk mengangkat HTTPError jika terjadi error\n",
    "            return response\n",
    "        except (requests.RequestException, requests.HTTPError) as e:\n",
    "            print(f\"Request failed: {e}. Retrying in {delay} seconds...\")\n",
    "            time.sleep(delay)\n",
    "            delay *= 2  # Penundaan eksponensial\n",
    "    raise Exception(\"Max retries exceeded\")\n",
    "\n",
    "\n",
    "def extract_article_links(index_url):\n",
    "    links = []\n",
    "    while True:\n",
    "        response = request_with_retries(index_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        articles = soup.find_all('h2', class_='title')\n",
    "        for article in articles:\n",
    "            a_tag = article.find('a')\n",
    "            link = a_tag.get('href')\n",
    "            if link and link.startswith('/'):\n",
    "                link = f'https://www.tempo.co{link}'\n",
    "            if link not in links:\n",
    "                links.append(link)\n",
    "\n",
    "        next_page = soup.find('a', class_='next')\n",
    "        if next_page and 'href' in next_page.attrs:\n",
    "            index_url = f'https://www.tempo.co{next_page[\"href\"]}'\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        time.sleep(random.uniform(3, 7))  # Randomized delay\n",
    "\n",
    "    return links\n",
    "\n",
    "def extract_article_data(url):\n",
    "    response = request_with_retries(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Ekstrak judul\n",
    "    title_tag = soup.find('h1', class_='title margin-bottom-sm')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else ''\n",
    "\n",
    "    # Ekstrak reporter dan editor\n",
    "    block_avatar = soup.find('div', class_='block-avatar')\n",
    "    reporter, editor = '', ''\n",
    "    if block_avatar:\n",
    "        title_tags = block_avatar.find_all('p', class_='title bold')\n",
    "        if len(title_tags) > 0:\n",
    "            reporter = title_tags[0].get_text(strip=True)\n",
    "        if len(title_tags) > 1:\n",
    "            editor = title_tags[1].get_text(strip=True)\n",
    "\n",
    "    # Ekstrak tanggal dan waktu\n",
    "    date_time_str = soup.find('p', class_='date margin-bottom-sm')\n",
    "    date_time = ''\n",
    "    if date_time_str:\n",
    "        date_time_str = date_time_str.get_text(strip=True)\n",
    "        try:\n",
    "            date_time = datetime.strptime(date_time_str, '%A, %d %B %Y %H:%M WIB')\n",
    "        except ValueError:\n",
    "            date_time = date_time_str\n",
    "\n",
    "    # Ekstrak konten\n",
    "    content_div = soup.find('div', class_='detail-konten')\n",
    "    content = []\n",
    "    if content_div:\n",
    "        paragraphs = content_div.find_all('p')\n",
    "        for p in paragraphs:\n",
    "            if p.find_parent(class_='bacajuga') or p.find_parent(class_='iklan') or 'Pilihan Editor' in p.text:\n",
    "                continue\n",
    "            content.append(p.get_text(strip=True))\n",
    "\n",
    "    content = ' '.join(content)\n",
    "\n",
    "    return {\n",
    "        'title': title,\n",
    "        'reporter': reporter,\n",
    "        'editor': editor,\n",
    "        'date_time': date_time,\n",
    "        'content': content\n",
    "    }\n",
    "\n",
    "\n",
    "def load_existing_articles(file_path):\n",
    "    try:\n",
    "        df_existing = pd.read_csv(file_path)\n",
    "        if df_existing.empty:\n",
    "            print(\"Existing CSV is empty. Starting fresh.\")\n",
    "            return pd.DataFrame(columns=['title', 'reporter', 'editor', 'date_time', 'content'])\n",
    "        return df_existing\n",
    "    except FileNotFoundError:\n",
    "        print(\"CSV file not found. Creating a new one.\")\n",
    "        return pd.DataFrame(columns=['title', 'reporter', 'editor', 'date_time', 'content'])\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"CSV file is empty. Creating a new DataFrame.\")\n",
    "        return pd.DataFrame(columns=['title', 'reporter', 'editor', 'date_time', 'content'])\n",
    "    \n",
    "def crawl_articles(start_date, end_date, max_articles=50):\n",
    "    date_format = '%Y-%m-%d'\n",
    "\n",
    "    # Load existing data from the CSV file\n",
    "    existing_df = load_existing_articles('articles_data.csv')\n",
    "    existing_articles = set(zip(existing_df['title'], existing_df['date_time']))\n",
    "\n",
    "    # Counter for newly crawled articles\n",
    "    new_articles_count = 0\n",
    "\n",
    "    current_date = datetime.strptime(start_date, date_format)\n",
    "\n",
    "    while new_articles_count < max_articles and current_date >= datetime.strptime(end_date, date_format):\n",
    "        index_url = f'https://www.tempo.co/indeks/{current_date.strftime(date_format)}/tekno'\n",
    "        print(f\"Crawling articles from {index_url}\")\n",
    "\n",
    "        article_links = extract_article_links(index_url)\n",
    "\n",
    "        if not article_links:\n",
    "            print(\"No more articles found.\")\n",
    "            break\n",
    "\n",
    "        for link in article_links:\n",
    "            if new_articles_count >= max_articles:\n",
    "                break\n",
    "            try:\n",
    "                # Extract data from the article\n",
    "                data = extract_article_data(link)\n",
    "                article_id = (data['title'], data['date_time'])\n",
    "\n",
    "                # Only save if the article is new\n",
    "                if article_id not in existing_articles:\n",
    "                    df_new = pd.DataFrame([data])\n",
    "                    df_new.to_csv('articles_data.csv', mode='a', header=not existing_articles, index=False)\n",
    "                    existing_articles.add(article_id)\n",
    "                    new_articles_count += 1\n",
    "                    print(f\"Saved article: {data['title']}\")\n",
    "                else:\n",
    "                    print(f\"Duplicate article found: {data['title']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {link}: {e}\")\n",
    "            time.sleep(random.uniform(3, 7))  # Randomized delay\n",
    "\n",
    "        # Move to the previous date\n",
    "        current_date -= timedelta(days=1)\n",
    "\n",
    "\n",
    "# sesuaikan dengan keperluan\n",
    "start_date = '2024-05-17'\n",
    "end_date = '2024-01-17'\n",
    "max_articles = 10\n",
    "\n",
    "crawl_articles(start_date, end_date, max_articles)\n",
    "print(f\"Data telah disimpan ke 'articles_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pembersihan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, to_date, concat_ws\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "# Membuat SparkSession\n",
    "spark = SparkSession.builder.appName(\"PreprocessingNewsData\").getOrCreate()\n",
    "\n",
    "# Membaca data dari file CSV\n",
    "df = spark.read.csv(\"hdfs://hadoop-namenode:8020/user/news_data/articles_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Membersihkan teks dengan menghapus karakter khusus dari kolom title, reporter, editor, dan content\n",
    "df_clean = df.withColumn(\"title\", regexp_replace(col(\"title\"), \"[^a-zA-Z0-9\\\\s]\", \"\")) \\\n",
    "             .withColumn(\"reporter\", regexp_replace(col(\"reporter\"), \"[^a-zA-Z0-9\\\\s]\", \"\")) \\\n",
    "             .withColumn(\"editor\", regexp_replace(col(\"editor\"), \"[^a-zA-Z0-9\\\\s]\", \"\")) \\\n",
    "             .withColumn(\"content\", regexp_replace(col(\"content\"), \"[^a-zA-Z0-9\\\\s]\", \"\"))\n",
    "\n",
    "# Konversi kolom date_time ke format tanggal\n",
    "df_clean = df_clean.withColumn(\"date_time\", to_date(col(\"date_time\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Tokenisasi teks pada kolom title dan content\n",
    "tokenizer_title = Tokenizer(inputCol=\"title\", outputCol=\"title_words\")\n",
    "tokenizer_content = Tokenizer(inputCol=\"content\", outputCol=\"content_words\")\n",
    "\n",
    "df_tokenized = tokenizer_title.transform(df_clean)\n",
    "df_tokenized = tokenizer_content.transform(df_tokenized)\n",
    "\n",
    "# Menghapus stopwords dari kolom title dan content\n",
    "remover_title = StopWordsRemover(inputCol=\"title_words\", outputCol=\"title_filtered\")\n",
    "remover_content = StopWordsRemover(inputCol=\"content_words\", outputCol=\"content_filtered\")\n",
    "\n",
    "df_preprocessed = remover_title.transform(df_tokenized)\n",
    "df_preprocessed = remover_content.transform(df_preprocessed)\n",
    "\n",
    "# Mengonversi kolom array menjadi string menggunakan concat_ws\n",
    "df_result = df_preprocessed.withColumn(\"title_filtered\", concat_ws(\" \", col(\"title_filtered\"))) \\\n",
    "                           .withColumn(\"content_filtered\", concat_ws(\" \", col(\"content_filtered\")))\n",
    "\n",
    "# Menampilkan hasil akhir preprocessing\n",
    "df_result.select(\"title_filtered\", \"reporter\", \"editor\", \"date_time\", \"content_filtered\").show(5, truncate=False)\n",
    "\n",
    "# Menyimpan hasil preprocessing ke HDFS sebagai CSV\n",
    "output_path = \"hdfs://hadoop-namenode:8020/user/news_data/hasilpreprocessing.csv\"\n",
    "\n",
    "df_result.select(\"title_filtered\", \"reporter\", \"editor\", \"date_time\", \"content_filtered\") \\\n",
    "         .write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "#df = spark.read.csv(\"hdfs://hadoop-namenode:8020/user/news_data/articles_data.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pemrosesan, Modeling, dan Visualisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import col, concat_ws, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Membuat SparkSession\n",
    "spark = SparkSession.builder.appName(\"TFIDFClusteringWithSentiment\").getOrCreate()\n",
    "\n",
    "# --- 1. Membaca Data ---\n",
    "input_path = \"hdfs://hadoop-namenode:8020/user/news_data/hasilpreprocessing.csv\"\n",
    "df_result = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "\n",
    "# Menggabungkan kata-kata yang telah difilter dari kolom title dan content\n",
    "df_filtered = df_result.withColumn(\"filtered_words\", concat_ws(\" \", col(\"title_filtered\"), col(\"content_filtered\")))\n",
    "\n",
    "# Menampilkan 5 baris teratas setelah menggabungkan kata-kata yang difilter\n",
    "df_filtered.select(\"filtered_words\").show(5, truncate=False)\n",
    "\n",
    "# --- 2. Tokenisasi dan TF-IDF ---\n",
    "# Tokenisasi\n",
    "tokenizer_filtered = Tokenizer(inputCol=\"filtered_words\", outputCol=\"words\")\n",
    "df_tokenized_filtered = tokenizer_filtered.transform(df_filtered)\n",
    "\n",
    "# Menampilkan 5 baris teratas setelah tokenisasi\n",
    "df_tokenized_filtered.select(\"filtered_words\", \"words\").show(5, truncate=False)\n",
    "\n",
    "# HashingTF\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "featurizedData = hashingTF.transform(df_tokenized_filtered)\n",
    "\n",
    "# Menampilkan 5 baris teratas setelah HashingTF\n",
    "featurizedData.select(\"words\", \"rawFeatures\").show(5, truncate=False)\n",
    "\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "# Menampilkan 5 baris teratas setelah IDF\n",
    "rescaledData.select(\"rawFeatures\", \"features\").show(5, truncate=False)\n",
    "\n",
    "# Konversi kolom features menjadi array float\n",
    "def vector_to_array(v):\n",
    "    return v.toArray().tolist()\n",
    "\n",
    "vector_to_array_udf = udf(vector_to_array, ArrayType(FloatType()))\n",
    "rescaledData = rescaledData.withColumn(\"features_array\", vector_to_array_udf(col(\"features\")))\n",
    "\n",
    "# --- 3. Menentukan Sentimen ---\n",
    "positive_words = ['baik', 'bagus', 'positif', 'hebat', 'senang', 'puas', 'luar biasa']\n",
    "negative_words = ['buruk', 'negatif', 'jelek', 'sedih', 'kecewa', 'mengecewakan', 'payah']\n",
    "\n",
    "def determine_sentiment(text):\n",
    "    text_lower = text.lower()\n",
    "    if any(word in text_lower for word in positive_words):\n",
    "        return 'positive'\n",
    "    elif any(word in text_lower for word in negative_words):\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "determine_sentiment_udf = udf(determine_sentiment, StringType())\n",
    "df_with_sentiment = rescaledData.withColumn(\"sentiment\", determine_sentiment_udf(col(\"filtered_words\")))\n",
    "\n",
    "# --- 4. K-Means Clustering ---\n",
    "kmeans = KMeans(k=3, seed=1, featuresCol=\"features\", predictionCol=\"cluster\")\n",
    "model = kmeans.fit(df_with_sentiment)\n",
    "df_with_cluster = model.transform(df_with_sentiment)\n",
    "\n",
    "# Evaluasi clustering\n",
    "evaluator = ClusteringEvaluator(predictionCol=\"cluster\", featuresCol=\"features\")\n",
    "silhouette = evaluator.evaluate(df_with_cluster)\n",
    "print(f\"Silhouette Score: {silhouette}\")\n",
    "\n",
    "# --- 5. PCA untuk Visualisasi ---\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "df_pca = pca.fit(df_with_cluster).transform(df_with_cluster)\n",
    "\n",
    "# Ekstraksi nilai PCA\n",
    "def extract_pca_values(vector):\n",
    "    return vector.toArray().tolist()\n",
    "\n",
    "extract_pca_values_udf = udf(extract_pca_values, ArrayType(FloatType()))\n",
    "df_pca = df_pca.withColumn(\"pca_values\", extract_pca_values_udf(col(\"pca_features\")))\n",
    "\n",
    "# Ambil komponen PCA pertama dan kedua\n",
    "df_pca = df_pca.withColumn(\"x\", col(\"pca_values\")[0])\n",
    "df_pca = df_pca.withColumn(\"y\", col(\"pca_values\")[1])\n",
    "\n",
    "# Konversi ke Pandas untuk visualisasi\n",
    "df_pca_pd = df_pca.select(\"x\", \"y\", \"cluster\", \"sentiment\").toPandas()\n",
    "\n",
    "# --- 6. Visualisasi Hasil Clustering dengan Sentimen ---\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Scatter plot dengan warna berdasarkan cluster\n",
    "scatter = plt.scatter(df_pca_pd['x'], df_pca_pd['y'], c=df_pca_pd['cluster'], cmap='viridis', marker='o')\n",
    "\n",
    "# Tambahkan judul dan label sumbu\n",
    "plt.title(\"Visualisasi Hasil Clustering dengan K-Means dan Sentimen (PCA 2D)\")\n",
    "plt.xlabel(\"PCA Komponen 1\")\n",
    "plt.ylabel(\"PCA Komponen 2\")\n",
    "\n",
    "# Tambahkan colorbar untuk cluster\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "\n",
    "# Simpan plot sebagai file PNG\n",
    "plt.savefig(\"clustering_sentiment_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "# --- 7. Visualisasi Sentimen dalam Bentuk Pie Chart ---\n",
    "\n",
    "# Hitung jumlah sentimen\n",
    "sentiment_counts = df_pca_pd['sentiment'].value_counts()\n",
    "\n",
    "# Membuat pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140, colors=['#66b3ff', '#ff9999', '#99ff99'])\n",
    "plt.title(\"Distribusi Sentimen dalam Clustering\")\n",
    "\n",
    "# Simpan pie chart sebagai file PNG\n",
    "plt.savefig(\"sentiment_distribution_pie.png\")\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
