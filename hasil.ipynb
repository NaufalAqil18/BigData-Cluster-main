{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pemrosesan Data\n",
    "\n",
    "## pembersihan data dan transformasi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, to_date, concat_ws\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "# Membuat SparkSession\n",
    "spark = SparkSession.builder.appName(\"PreprocessingNewsData\").getOrCreate()\n",
    "\n",
    "# Membaca data dari file CSV\n",
    "df = spark.read.csv(\"hdfs://hadoop-namenode:8020/user/news_data/articles_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Membersihkan teks dengan menghapus karakter khusus dari kolom title, reporter, editor, dan content\n",
    "df_clean = df.withColumn(\"title\", regexp_replace(col(\"title\"), \"[^a-zA-Z0-9\\\\s]\", \"\")) \\\n",
    "             .withColumn(\"reporter\", regexp_replace(col(\"reporter\"), \"[^a-zA-Z0-9\\\\s]\", \"\")) \\\n",
    "             .withColumn(\"editor\", regexp_replace(col(\"editor\"), \"[^a-zA-Z0-9\\\\s]\", \"\")) \\\n",
    "             .withColumn(\"content\", regexp_replace(col(\"content\"), \"[^a-zA-Z0-9\\\\s]\", \"\"))\n",
    "\n",
    "# Konversi kolom date_time ke format tanggal\n",
    "df_clean = df_clean.withColumn(\"date_time\", to_date(col(\"date_time\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Tokenisasi teks pada kolom title dan content\n",
    "tokenizer_title = Tokenizer(inputCol=\"title\", outputCol=\"title_words\")\n",
    "tokenizer_content = Tokenizer(inputCol=\"content\", outputCol=\"content_words\")\n",
    "\n",
    "df_tokenized = tokenizer_title.transform(df_clean)\n",
    "df_tokenized = tokenizer_content.transform(df_tokenized)\n",
    "\n",
    "# Menghapus stopwords dari kolom title dan content\n",
    "remover_title = StopWordsRemover(inputCol=\"title_words\", outputCol=\"title_filtered\")\n",
    "remover_content = StopWordsRemover(inputCol=\"content_words\", outputCol=\"content_filtered\")\n",
    "\n",
    "df_preprocessed = remover_title.transform(df_tokenized)\n",
    "df_preprocessed = remover_content.transform(df_preprocessed)\n",
    "\n",
    "# Mengonversi kolom array menjadi string menggunakan concat_ws\n",
    "df_result = df_preprocessed.withColumn(\"title_filtered\", concat_ws(\" \", col(\"title_filtered\"))) \\\n",
    "                           .withColumn(\"content_filtered\", concat_ws(\" \", col(\"content_filtered\")))\n",
    "\n",
    "# Menampilkan hasil akhir preprocessing\n",
    "df_result.select(\"title_filtered\", \"reporter\", \"editor\", \"date_time\", \"content_filtered\").show(5, truncate=False)\n",
    "\n",
    "# Menyimpan hasil preprocessing ke HDFS sebagai CSV\n",
    "output_path = \"hdfs://hadoop-namenode:8020/user/news_data/hasilpreprocessing.csv\"\n",
    "\n",
    "df_result.select(\"title_filtered\", \"reporter\", \"editor\", \"date_time\", \"content_filtered\") \\\n",
    "         .write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "#df = spark.read.csv(\"hdfs://hadoop-namenode:8020/user/news_data/articles_data.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pemodelan\n",
    "## Analisis Tren Kata Kunci Menggunakan TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.sql.functions import col, concat_ws, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Membuat SparkSession\n",
    "spark = SparkSession.builder.appName(\"TFIDFProcessing\").getOrCreate()\n",
    "\n",
    "# Membaca data hasil preprocessing dari file CSV\n",
    "input_path = \"hdfs://hadoop-namenode:8020/user/news_data/hasilpreprocessing.csv\"\n",
    "df_result = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "\n",
    "# Menggabungkan kata-kata yang telah difilter dari kolom title dan content\n",
    "df_filtered = df_result.withColumn(\"filtered_words\", concat_ws(\" \", col(\"title_filtered\"), col(\"content_filtered\")))\n",
    "\n",
    "# Tokenisasi teks dari kolom filtered_words\n",
    "tokenizer_filtered = Tokenizer(inputCol=\"filtered_words\", outputCol=\"words\")\n",
    "df_tokenized_filtered = tokenizer_filtered.transform(df_filtered)\n",
    "\n",
    "# Menghitung Term Frequency (TF)\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "featurizedData = hashingTF.transform(df_tokenized_filtered)\n",
    "\n",
    "# Menghitung Inverse Document Frequency (IDF)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "# Konversi kolom features (vektor) menjadi array float\n",
    "def vector_to_array(v):\n",
    "    return v.toArray().tolist()\n",
    "\n",
    "vector_to_array_udf = udf(vector_to_array, ArrayType(FloatType()))\n",
    "rescaledData = rescaledData.withColumn(\"features_array\", vector_to_array_udf(col(\"features\")))\n",
    "\n",
    "# Mengonversi array menjadi string (separated by commas)\n",
    "rescaledData = rescaledData.withColumn(\"features_string\", concat_ws(\",\", col(\"features_array\")))\n",
    "\n",
    "# Menampilkan hasil TF-IDF\n",
    "rescaledData.select(\"filtered_words\", \"features_string\").show(5)\n",
    "\n",
    "# Menyimpan hasil TF-IDF ke HDFS sebagai CSV\n",
    "output_path = \"hdfs://hadoop-namenode:8020/user/news_data/hasil_tfidf.csv\"\n",
    "rescaledData.select(\"filtered_words\", \"features_string\") \\\n",
    "            .write.csv(output_path, header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis Sentimen Menggunakan Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# Menambahkan label sentimen (contoh sederhana untuk demonstrasi)\n",
    "from pyspark.sql.functions import when\n",
    "df_labeled = df_filtered.withColumn(\"label\", \n",
    "            when(df_filtered[\"title\"].contains(\"baik\"), 1)\n",
    "            .when(df_filtered[\"title\"].contains(\"buruk\"), -1)\n",
    "            .otherwise(0))\n",
    "\n",
    "# Mengubah teks menjadi fitur dengan CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"features\")\n",
    "cv_model = cv.fit(df_labeled)\n",
    "df_vectorized = cv_model.transform(df_labeled)\n",
    "\n",
    "# Membagi data menjadi train dan test\n",
    "train_data, test_data = df_vectorized.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Melatih model Naive Bayes\n",
    "nb = NaiveBayes()\n",
    "model = nb.fit(train_data)\n",
    "\n",
    "# Memprediksi data uji\n",
    "predictions = model.transform(test_data)\n",
    "predictions.select(\"filtered_words\", \"label\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluasi Model\n",
    "## Akurasi, Precision, Recall, dan F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Menghitung Akurasi\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "print(f\"Akurasi Model: {accuracy}\")\n",
    "\n",
    "# Menghitung Precision\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = precision_evaluator.evaluate(predictions)\n",
    "print(f\"Precision Model: {precision}\")\n",
    "\n",
    "# Menghitung Recall\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = recall_evaluator.evaluate(predictions)\n",
    "print(f\"Recall Model: {recall}\")\n",
    "\n",
    "# Menghitung F1-Score\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = f1_evaluator.evaluate(predictions)\n",
    "print(f\"F1-Score Model: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Mengonversi DataFrame ke RDD untuk MulticlassMetrics\n",
    "prediction_and_labels = predictions.select(\"prediction\", \"label\").rdd.map(lambda x: (float(x['prediction']), float(x['label'])))\n",
    "\n",
    "# Menghitung Confusion Matrix\n",
    "metrics = MulticlassMetrics(prediction_and_labels)\n",
    "confusion_matrix = metrics.confusionMatrix()\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisasi Hasil\n",
    "## Visualisasi Distribusi Sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Mengunduh hasil prediksi dari HDFS\n",
    "# hdfs dfs -get /user/username/news_data/sentiment_predictions ~/sentiment_predictions\n",
    "\n",
    "# Membaca data hasil prediksi\n",
    "df = pd.read_csv('~/sentiment_predictions/part-00000', names=['title', 'filtered_words', 'label', 'prediction'])\n",
    "\n",
    "# Menghitung distribusi sentimen\n",
    "sentiment_counts = df['prediction'].value_counts()\n",
    "sentiment_labels = ['Negatif (-1)', 'Netral (0)', 'Positif (1)']\n",
    "\n",
    "# Plot Pie Chart Sentimen\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(sentiment_counts, labels=sentiment_labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribusi Sentimen Berita')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud untuk Kata Kunci Populer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Menggabungkan semua kata dari kolom 'filtered_words'\n",
    "all_words = ' '.join(df['filtered_words'].dropna())\n",
    "\n",
    "# Membuat Word Cloud\n",
    "wordcloud = WordCloud(width=800, height=600, background_color='white').generate(all_words)\n",
    "\n",
    "# Menampilkan Word Cloud\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud Kata Kunci Populer')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
